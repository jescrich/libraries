"use strict";(self.webpackChunknestjs_toolkit_docs=self.webpackChunknestjs_toolkit_docs||[]).push([[7799],{4987:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>t,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"kafka/consumer","title":"Kafka Consumer","description":"The Consumer decorator provides enterprise-grade message consumption with advanced batch processing, key-based grouping, and automatic pressure management.","source":"@site/docs/kafka/consumer.md","sourceDirName":"kafka","slug":"/kafka/consumer","permalink":"/libraries/docs/kafka/consumer","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Configuration","permalink":"/libraries/docs/kafka/configuration"},"next":{"title":"Kafka Producer","permalink":"/libraries/docs/kafka/producer"}}');var a=s(4848),o=s(8453);const t={sidebar_position:3},i="Kafka Consumer",c={},l=[{value:"Basic Consumer",id:"basic-consumer",level:2},{value:"Simple Message Consumer",id:"simple-message-consumer",level:3},{value:"Consumer with Error Handling",id:"consumer-with-error-handling",level:3},{value:"Batch Processing",id:"batch-processing",level:2},{value:"Basic Batch Consumer",id:"basic-batch-consumer",level:3},{value:"Batch Consumer with Key Grouping",id:"batch-consumer-with-key-grouping",level:3},{value:"Advanced Consumer Features",id:"advanced-consumer-features",level:2},{value:"High-Volume Consumer with Pressure Management",id:"high-volume-consumer-with-pressure-management",level:3},{value:"Consumer with Custom Deserialization",id:"consumer-with-custom-deserialization",level:3},{value:"Idempotent Consumer",id:"idempotent-consumer",level:3},{value:"Consumer Configuration Options",id:"consumer-configuration-options",level:2},{value:"Complete Configuration Example",id:"complete-configuration-example",level:3},{value:"Message Processing Patterns",id:"message-processing-patterns",level:2},{value:"Sequential Processing",id:"sequential-processing",level:3},{value:"Parallel Processing with Coordination",id:"parallel-processing-with-coordination",level:3},{value:"Conditional Processing",id:"conditional-processing",level:3},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"Consumer with Metrics",id:"consumer-with-metrics",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"kafka-consumer",children:"Kafka Consumer"})}),"\n",(0,a.jsx)(n.p,{children:"The Consumer decorator provides enterprise-grade message consumption with advanced batch processing, key-based grouping, and automatic pressure management."}),"\n",(0,a.jsx)(n.h2,{id:"basic-consumer",children:"Basic Consumer"}),"\n",(0,a.jsx)(n.h3,{id:"simple-message-consumer",children:"Simple Message Consumer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"import { Consumer } from '@jescrich/nestjs-kafka-client';\nimport { Injectable } from '@nestjs/common';\n\n@Injectable()\n@Consumer('orders')\nexport class OrderConsumer {\n  async handleMessage(message: KafkaMessage) {\n    const order = JSON.parse(message.value.toString());\n    console.log('Processing order:', order);\n    \n    // Process the order\n    await this.processOrder(order);\n    \n    // Automatic commit after successful processing\n    // Built-in error handling with DLQ support\n  }\n\n  private async processOrder(order: any) {\n    // Your business logic here\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"consumer-with-error-handling",children:"Consumer with Error Handling"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('payments', {\n  dlq: {\n    topic: 'payments-dlq',\n    maxRetries: 3,\n    retryDelay: 1000, // 1 second between retries\n  }\n})\nexport class PaymentConsumer {\n  private readonly logger = new Logger(PaymentConsumer.name);\n\n  async handleMessage(message: KafkaMessage) {\n    try {\n      const payment = JSON.parse(message.value.toString());\n      await this.processPayment(payment);\n      this.logger.log(`Payment processed: ${payment.id}`);\n    } catch (error) {\n      this.logger.error(`Payment processing failed: ${error.message}`);\n      throw error; // Will trigger retry logic\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,a.jsx)(n.h3,{id:"basic-batch-consumer",children:"Basic Batch Consumer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('orders', {\n  batch: true,\n  batchSize: 100,\n  batchTimeout: 5000, // Process batch every 5 seconds or when full\n})\nexport class OrderBatchConsumer {\n  async handleBatch(messages: KafkaMessage[]) {\n    const orders = messages.map(msg => \n      JSON.parse(msg.value.toString())\n    );\n    \n    // Process all orders in the batch\n    await this.processBatchOrders(orders);\n  }\n\n  private async processBatchOrders(orders: any[]) {\n    // Efficient batch processing\n    await Promise.all(orders.map(order => this.processOrder(order)));\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"batch-consumer-with-key-grouping",children:"Batch Consumer with Key Grouping"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('orders', {\n  batch: true,\n  batchSize: 100,\n  batchTimeout: 5000,\n  groupByKey: true, // Group messages by key within batch\n})\nexport class OrderBatchConsumer {\n  async handleBatch(messages: KafkaMessage[]) {\n    // Messages are automatically grouped by key\n    // All messages for customer 'A' will be in sequence\n    const ordersByCustomer = this.groupByCustomer(messages);\n    \n    // Process each customer's orders in parallel\n    await Promise.all(\n      Object.entries(ordersByCustomer).map(([customerId, orders]) =>\n        this.processCustomerOrders(customerId, orders)\n      )\n    );\n  }\n\n  private groupByCustomer(messages: KafkaMessage[]) {\n    return messages.reduce((acc, msg) => {\n      const customerId = msg.key?.toString();\n      if (!acc[customerId]) acc[customerId] = [];\n      acc[customerId].push(JSON.parse(msg.value.toString()));\n      return acc;\n    }, {} as Record<string, any[]>);\n  }\n\n  private async processCustomerOrders(customerId: string, orders: any[]) {\n    // Process orders for a specific customer in sequence\n    for (const order of orders) {\n      await this.processOrder(order);\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"advanced-consumer-features",children:"Advanced Consumer Features"}),"\n",(0,a.jsx)(n.h3,{id:"high-volume-consumer-with-pressure-management",children:"High-Volume Consumer with Pressure Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('high-volume-topic', {\n  batch: true,\n  batchSize: 500,\n  maxConcurrency: 10,        // Limit concurrent batch processing\n  backPressureThreshold: 80, // Pause consumption at 80% capacity\n  idempotencyKey: (msg) => msg.headers['idempotency-key'], // Custom idempotency\n})\nexport class HighVolumeConsumer {\n  async handleBatch(messages: KafkaMessage[]) {\n    // Automatic back pressure management\n    // If processing falls behind, consumption will pause\n    // Front pressure is managed through intelligent buffering\n    \n    await this.processMessages(messages);\n  }\n\n  private async processMessages(messages: KafkaMessage[]) {\n    // Your high-volume processing logic\n    const chunks = this.chunkArray(messages, 50);\n    \n    for (const chunk of chunks) {\n      await Promise.all(\n        chunk.map(msg => this.processMessage(msg))\n      );\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"consumer-with-custom-deserialization",children:"Consumer with Custom Deserialization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('avro-orders', {\n  deserializer: 'avro',\n})\nexport class AvroOrderConsumer {\n  constructor(private readonly avroDeserializer: Deserializer) {}\n\n  async handleMessage(message: KafkaMessage) {\n    // Automatic deserialization based on schema\n    const order = await this.avroDeserializer.deserialize(\n      'order-schema',\n      message.value\n    );\n    \n    await this.processOrder(order);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"idempotent-consumer",children:"Idempotent Consumer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('payments', {\n  idempotencyKey: (message) => message.headers['transaction-id'],\n  idempotencyTtl: 3600000, // 1 hour\n})\nexport class PaymentConsumer {\n  async handleMessage(message: KafkaMessage) {\n    // This message will only be processed once per transaction-id\n    // Duplicates are automatically filtered out\n    \n    const payment = JSON.parse(message.value.toString());\n    await this.processPayment(payment);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"consumer-configuration-options",children:"Consumer Configuration Options"}),"\n",(0,a.jsx)(n.h3,{id:"complete-configuration-example",children:"Complete Configuration Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('complex-topic', {\n  // Batch processing\n  batch: true,\n  batchSize: 200,\n  batchTimeout: 10000,\n  \n  // Key grouping and ordering\n  groupByKey: true,\n  \n  // Pressure management\n  maxConcurrency: 5,\n  backPressureThreshold: 80,\n  \n  // Idempotency\n  idempotencyKey: (msg) => msg.headers['id'],\n  idempotencyTtl: 3600000,\n  \n  // Error handling\n  dlq: {\n    topic: 'complex-topic-dlq',\n    maxRetries: 3,\n    retryDelay: 1000,\n  },\n  \n  // Consumer group settings\n  groupId: 'complex-consumer-group',\n  sessionTimeout: 30000,\n  heartbeatInterval: 3000,\n  \n  // Offset management\n  fromBeginning: false,\n  autoCommit: true,\n  autoCommitInterval: 5000,\n})\nexport class ComplexConsumer {\n  async handleBatch(messages: KafkaMessage[]) {\n    // Your processing logic\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"message-processing-patterns",children:"Message Processing Patterns"}),"\n",(0,a.jsx)(n.h3,{id:"sequential-processing",children:"Sequential Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('sequential-orders')\nexport class SequentialOrderConsumer {\n  async handleMessage(message: KafkaMessage) {\n    const order = JSON.parse(message.value.toString());\n    \n    // Process steps in sequence\n    await this.validateOrder(order);\n    await this.reserveInventory(order);\n    await this.processPayment(order);\n    await this.fulfillOrder(order);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"parallel-processing-with-coordination",children:"Parallel Processing with Coordination"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('parallel-orders', {\n  batch: true,\n  batchSize: 50,\n})\nexport class ParallelOrderConsumer {\n  async handleBatch(messages: KafkaMessage[]) {\n    const orders = messages.map(msg => JSON.parse(msg.value.toString()));\n    \n    // Process validation in parallel\n    await Promise.all(orders.map(order => this.validateOrder(order)));\n    \n    // Process payments in parallel\n    await Promise.all(orders.map(order => this.processPayment(order)));\n    \n    // Fulfill orders sequentially (if order matters)\n    for (const order of orders) {\n      await this.fulfillOrder(order);\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"conditional-processing",children:"Conditional Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('mixed-events')\nexport class EventConsumer {\n  async handleMessage(message: KafkaMessage) {\n    const event = JSON.parse(message.value.toString());\n    \n    switch (event.type) {\n      case 'order_created':\n        await this.handleOrderCreated(event);\n        break;\n      case 'payment_processed':\n        await this.handlePaymentProcessed(event);\n        break;\n      case 'order_shipped':\n        await this.handleOrderShipped(event);\n        break;\n      default:\n        console.log(`Unknown event type: ${event.type}`);\n    }\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,a.jsx)(n.h3,{id:"consumer-with-metrics",children:"Consumer with Metrics"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"@Consumer('monitored-topic')\nexport class MonitoredConsumer {\n  private readonly logger = new Logger(MonitoredConsumer.name);\n  private processedCount = 0;\n  private errorCount = 0;\n\n  async handleMessage(message: KafkaMessage) {\n    const startTime = Date.now();\n    \n    try {\n      await this.processMessage(message);\n      this.processedCount++;\n      \n      const duration = Date.now() - startTime;\n      this.logger.log(`Message processed in ${duration}ms`);\n    } catch (error) {\n      this.errorCount++;\n      this.logger.error(`Processing failed: ${error.message}`);\n      throw error;\n    }\n  }\n\n  @Cron('0 * * * * *') // Every minute\n  logMetrics() {\n    this.logger.log(`Processed: ${this.processedCount}, Errors: ${this.errorCount}`);\n  }\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use batch processing"})," for high-throughput scenarios"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enable key grouping"})," when message order matters within a key"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Configure appropriate batch sizes"})," based on your processing capacity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement idempotency"})," for critical business operations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Set up DLQ"})," for proper error handling"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Monitor consumer lag"})," and processing metrics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use back pressure"})," to prevent system overload"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Handle errors gracefully"})," with proper logging"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"./advanced-features",children:"Explore Advanced Features"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"./configuration",children:"Review Configuration Options"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"./best-practices",children:"Learn Best Practices"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>i});var r=s(6540);const a={},o=r.createContext(a);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);